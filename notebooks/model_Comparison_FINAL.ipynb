{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4223956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0466ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 1. COLLECT FINAL PERFORMANCE METRICS\n",
    "# =================================================================\n",
    "# Objective: Consolidate final R2 and MAE scores from all competing models\n",
    "# Note: Update competitor scores (Decision Tree, Lasso, Linear, KNN) \n",
    "#       with actual metrics from team members' notebooks before running.\n",
    "\n",
    "model_data = {\n",
    "    'Model Name': [\n",
    "        'Random Forest Regressor (Pritilata)',\n",
    "        'Decision Tree Regressor (Dhrubajit)',\n",
    "        'Lasso Regression (Shriyut)',\n",
    "        'Linear Regression (Shriyut)',\n",
    "        'KNN Regressor (Subhadip)'\n",
    "    ],\n",
    "    'R2_Score': [\n",
    "        0.9789,  # Champion Score (Updated to actual verified R2)\n",
    "        0.9769,  # Competitor Score (Update this example value)\n",
    "        0.8175,  # Competitor Score (Update this example value)\n",
    "        0.8176,  # Competitor Score (Update this example value)\n",
    "        0.9724   # Competitor Score (Update this example value)\n",
    "    ],\n",
    "    'MAE': [\n",
    "        847.67,  # Champion MAE (Updated to actual verified MAE)\n",
    "        379.02, # Competitor MAE (Update this example value)\n",
    "        3674.31, # Competitor MAE (Update this example value)\n",
    "        3674.17, # Competitor MAE (Update this example value)\n",
    "        1083.87  # Competitor MAE (Update this example value)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a63aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Comparison Table:\n",
      "------------------------------------------------------------\n",
      "                         Model Name R2_Score_Formatted     MAE              Status\n",
      "Random Forest Regressor (Pritilata)             97.89%  847.67 üèÜ CHAMPION ARTIFACT\n",
      "Decision Tree Regressor (Dhrubajit)             97.69%  379.02          Competitor\n",
      "           KNN Regressor (Subhadip)             97.24% 1083.87          Competitor\n",
      "        Linear Regression (Shriyut)             81.76% 3674.17          Competitor\n",
      "         Lasso Regression (Shriyut)             81.75% 3674.31          Competitor\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 2. RANKING AND ARTIFACT IDENTIFICATION\n",
    "# =================================================================\n",
    "# Rank models by R2 score in descending order (highest R2 is best)\n",
    "df_comparison = df_comparison.sort_values(by='R2_Score', ascending=False)\n",
    "\n",
    "# Format R2 score for presentation (e.g., 0.9789 -> '97.89%')\n",
    "df_comparison['R2_Score_Formatted'] = (df_comparison['R2_Score'] * 100).round(2).astype(str) + '%'\n",
    "\n",
    "# Identify the officially selected Champion Model\n",
    "# Note: The R2 score in the condition MUST match the Champion R2_Score value above (0.9789)\n",
    "df_comparison['Status'] = np.where(df_comparison['R2_Score'] == 0.9789, 'üèÜ CHAMPION ARTIFACT', 'Competitor')\n",
    "\n",
    "# Select and order the columns for final display\n",
    "df_final = df_comparison[['Model Name', 'R2_Score_Formatted', 'MAE', 'Status']]\n",
    "\n",
    "# Print the comparison table in Markdown format for easy documentation update\n",
    "print(\"Final Model Comparison Table:\")\n",
    "print(\"-\" * 60)\n",
    "print(df_final.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c51a1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created missing directory: ../data/processed\n",
      "‚úÖ Comparison data successfully saved at: ../data/processed\\final_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "import os  # Import OS library to handle folder paths.\n",
    "\n",
    "# =================================================================\n",
    "# 3. PERSISTENCE (Saving Comparison Data)\n",
    "# =================================================================\n",
    "# Objective: Save the DataFrame to a CSV file, ensuring the directory exists.\n",
    "\n",
    "# Define the output directory and file name\n",
    "output_dir = '../data/processed'\n",
    "file_name = 'final_model_comparison.csv'\n",
    "full_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# CRITICAL STEP: Check if directory exists; if not, create it.\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"üìÅ Created missing directory: {output_dir}\")\n",
    "\n",
    "# Save the complete comparison DataFrame\n",
    "try:\n",
    "    df_comparison.to_csv(full_path, index=False)\n",
    "    print(f\"‚úÖ Comparison data successfully saved at: {full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
